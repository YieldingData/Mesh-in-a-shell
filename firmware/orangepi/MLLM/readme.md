<p align="center">
  <img src="../../../assets/MLLM.png" alt="MLLM Logo" width="400">
</p>
# MLLM â€“ Mesh Local Language Model

**MLLM** (Mesh Local Language Model) is the embedded AI module within the *Mesh in a Shell* system. It provides local, offline language processing for composing messages, parsing commands, and rephrasing input using a small quantized LLM running directly on the Orange Pi Zero 2 W.

---

## ğŸ§  Purpose

MLLM brings basic language assistance features to constrained hardware without relying on cloud APIs. It supports:

- âœï¸ Text autocompletion for messages
- ğŸ”„ Rephrasing and simplification of input
- ğŸ“œ Natural language command parsing
- ğŸ§­ Local assistant functionality for user queries (planned)

---

## ğŸ“ Directory Structure

```
mllm/
â”œâ”€â”€ __init__.py           # Entry point and high-level interface
â”œâ”€â”€ model_runner.py       # Loads and runs the LLM (TinyLlama)
â”œâ”€â”€ completion.py         # Handles autocomplete prompts
â”œâ”€â”€ rephrase.py           # Rewrites user input in different styles
â”œâ”€â”€ command_parser.py     # Parses structured commands from user text
â””â”€â”€ utils.py              # Shared tools for formatting and token control
```

---

## ğŸ›  Requirements

- `llama.cpp` compiled for ARM64
- GGUF model (default: TinyLlama 1.1B Q4_K_M)
- Python 3.8+
- Optional: `pyllamacpp` or subprocess wrapper to interact with compiled binary

---

## ğŸ“¦ Default Model

| Model       | TinyLlama 1.1B Chat |
|-------------|---------------------|
| Format      | GGUF (Q4_K_M)       |
| Size        | ~140MB              |
| RAM Usage   | ~450MB              |
| Token Rate  | ~5â€“10 tokens/sec    |
| License     | Apache 2.0          |

---

## ğŸ§© Integration Points

MLLM is designed to interface with:

- `keys/` â†’ Accepts typed input from the on-screen keyboard
- `messages/` â†’ Suggests completions and cleans up outgoing text
- `config/` â†’ Accepts natural language configuration updates (via parser)

---

## ğŸ§ª Usage

Example call:

```python
from mllm import complete_text

response = complete_text("Let's meet at the")
print(response)
```

---

## ğŸ” Privacy

All inference runs locally. No text is sent to the cloud. MLLM does not retain any messages or user history.

---

## ğŸ§­ Roadmap

- [x] Integrate TinyLlama GGUF runner
- [ ] Add UI toggle for "LLM Assist" mode
- [ ] Introduce caching for recent completions
- [ ] Add whisper-to-LLM bridge for voice commands
- [ ] Expand assistant capabilities for offline help

---

## ğŸ“„ License

MIT License Â© Mesh in a Shell contributors
